

<!DOCTYPE html>

<html lang="en-US">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge">

  
    <title>Exercises - VNA2V-W21</title>

    
  

  <link rel="shortcut icon" href="http://localhost:4000/VNA2V-W21-handouts/favicon.ico" type="image/x-icon">

  <link rel="stylesheet" href="http://localhost:4000/VNA2V-W21-handouts/assets/css/just-the-docs.css">
  <script src="https://use.fontawesome.com/d81b2d50d8.js"></script>
  
  

  
    <script type="text/javascript" src="http://localhost:4000/VNA2V-W21-handouts/assets/js/vendor/lunr.min.js"></script>
  
  <script type="text/javascript" src="http://localhost:4000/VNA2V-W21-handouts/assets/js/just-the-docs.js"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Exercises | VNA2V-W21</title>
<meta name="generator" content="Jekyll v3.8.6" />
<meta property="og:title" content="Exercises" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="AEROSP 740 - Visual Navigation for Autonomous Aerial Vehicles (W21)" />
<meta property="og:description" content="AEROSP 740 - Visual Navigation for Autonomous Aerial Vehicles (W21)" />
<link rel="canonical" href="http://localhost:4000/VNA2V-W21-handouts/lab9/exercises" />
<meta property="og:url" content="http://localhost:4000/VNA2V-W21-handouts/lab9/exercises" />
<meta property="og:site_name" content="VNA2V-W21" />
<script type="application/ld+json">
{"@type":"WebPage","headline":"Exercises","description":"AEROSP 740 - Visual Navigation for Autonomous Aerial Vehicles (W21)","url":"http://localhost:4000/VNA2V-W21-handouts/lab9/exercises","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


  
<script>
  window.MathJax = {
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    },
    loader: {
      load: ['input/tex', '[tex]/ams', '[tex]/configMacros']
    },
    tex: {
      packages: {'[+]': ['boldsymbol', 'ams', 'configMacros']},
      inlineMath: [['$','$'], ['\\(','\\)']],
      displayMath: [["$$", "$$"], ["\\[","\\]"]],
      processEscapes: true,
      tags: "ams",
      macros: {
        SE: ['\\mathrm{SE}(#1)', 1],
        SO: ['\\mathrm{SO}(#1)', 1],
        argmin: '\\mathop{\\operatorname{argmin}}',
        argmax: '\\mathop{\\operatorname{argmax}}',
        trace: '\\operatorname{trace}',
        tran: '^{\\mathsf{T}}',
      },
    },
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" async></script>


<script type="text/javascript" src="http://localhost:4000/VNA2V-W21-handouts/assets/js/vendor/lazysizes.min.js" async=""></script>

</head>

<body>

  <div class="page-wrap">
    <div class="side-bar">
      <div class="site-header">
        <a href="http://localhost:4000/VNA2V-W21-handouts" class="site-title lh-tight">VNA2V-W21</a>
        <button class="menu-button fs-3 js-main-nav-trigger" data-text-toggle="Hide" type="button">Menu</button>
      </div>

      <div class="navigation main-nav js-main-nav">
        <nav role="navigation" aria-label="Main navigation">
  <ul class="navigation-list">
<li class="navigation-list-item"><a href="http://localhost:4000/VNA2V-W21-handouts/" class="navigation-list-link">Home</a></li>
<li class="navigation-list-item">
<a href="http://localhost:4000/VNA2V-W21-handouts/lab1/" class="navigation-list-link">Lab 1</a><ul class="navigation-list-child-list ">
<li class="navigation-list-item "><a href="http://localhost:4000/VNA2V-W21-handouts/lab1/ubuntu" class="navigation-list-link">Install Ubuntu 18.04</a></li>
<li class="navigation-list-item "><a href="http://localhost:4000/VNA2V-W21-handouts/lab1/shell" class="navigation-list-link">Shell basics</a></li>
<li class="navigation-list-item "><a href="http://localhost:4000/VNA2V-W21-handouts/lab1/git" class="navigation-list-link">Git</a></li>
<li class="navigation-list-item "><a href="http://localhost:4000/VNA2V-W21-handouts/lab1/cpp" class="navigation-list-link">C++</a></li>
<li class="navigation-list-item "><a href="http://localhost:4000/VNA2V-W21-handouts/lab1/exercises" class="navigation-list-link">Exercises</a></li>
</ul>
</li>
<li class="navigation-list-item">
<a href="http://localhost:4000/VNA2V-W21-handouts/lab2/" class="navigation-list-link">Lab 2</a><ul class="navigation-list-child-list ">
<li class="navigation-list-item "><a href="http://localhost:4000/VNA2V-W21-handouts/lab2/ros" class="navigation-list-link">Installing ROS</a></li>
<li class="navigation-list-item "><a href="http://localhost:4000/VNA2V-W21-handouts/lab2/ros101" class="navigation-list-link">Introduction to ROS</a></li>
<li class="navigation-list-item "><a href="http://localhost:4000/VNA2V-W21-handouts/lab2/exercises" class="navigation-list-link">Exercises</a></li>
</ul>
</li>
<li class="navigation-list-item">
<a href="http://localhost:4000/VNA2V-W21-handouts/lab3/" class="navigation-list-link">Lab 3</a><ul class="navigation-list-child-list "><li class="navigation-list-item "><a href="http://localhost:4000/VNA2V-W21-handouts/lab3/exercises" class="navigation-list-link">Exercises</a></li></ul>
</li>
<li class="navigation-list-item">
<a href="http://localhost:4000/VNA2V-W21-handouts/lab4/" class="navigation-list-link">Lab 4</a><ul class="navigation-list-child-list "><li class="navigation-list-item "><a href="http://localhost:4000/VNA2V-W21-handouts/lab4/exercises" class="navigation-list-link">Exercises</a></li></ul>
</li>
<li class="navigation-list-item">
<a href="http://localhost:4000/VNA2V-W21-handouts/lab5/" class="navigation-list-link">Lab 5</a><ul class="navigation-list-child-list "><li class="navigation-list-item "><a href="http://localhost:4000/VNA2V-W21-handouts/lab5/exercises" class="navigation-list-link">Exercises</a></li></ul>
</li>
<li class="navigation-list-item">
<a href="http://localhost:4000/VNA2V-W21-handouts/lab6/" class="navigation-list-link">Lab 6</a><ul class="navigation-list-child-list "><li class="navigation-list-item "><a href="http://localhost:4000/VNA2V-W21-handouts/lab6/exercises" class="navigation-list-link">Exercises</a></li></ul>
</li>
<li class="navigation-list-item">
<a href="http://localhost:4000/VNA2V-W21-handouts/lab7/" class="navigation-list-link">Lab 7</a><ul class="navigation-list-child-list "><li class="navigation-list-item "><a href="http://localhost:4000/VNA2V-W21-handouts/lab7/exercises" class="navigation-list-link">Exercises</a></li></ul>
</li>
<li class="navigation-list-item">
<a href="http://localhost:4000/VNA2V-W21-handouts/lab8/" class="navigation-list-link">Lab 8</a><ul class="navigation-list-child-list "><li class="navigation-list-item "><a href="http://localhost:4000/VNA2V-W21-handouts/lab8/exercises" class="navigation-list-link">Exercises</a></li></ul>
</li>
<li class="navigation-list-item active">
<a href="http://localhost:4000/VNA2V-W21-handouts/lab9/" class="navigation-list-link">Lab 9</a><ul class="navigation-list-child-list "><li class="navigation-list-item  active"><a href="http://localhost:4000/VNA2V-W21-handouts/lab9/exercises" class="navigation-list-link active">Exercises</a></li></ul>
</li>
<li class="navigation-list-item"><a href="http://localhost:4000/VNA2V-W21-handouts/about" class="navigation-list-link">How to print</a></li>
</ul>
</nav>

      </div>
      <footer class="site-footer">
        <p class="text-small text-grey-dk-000 mb-4"><b>Last modified</b>:<br> Friday, January 15 at 14:43</p>
      </footer>
    </div>
    <div class="main-content-wrap js-main-content" tabindex="0">
      <div class="main-content">
        <div class="page-header js-page-header">
          
          <div class="search">
            <div class="search-input-wrap">
              <input type="text" class="js-search-input search-input" tabindex="0" placeholder="Search VNA2V-W21" aria-label="Search VNA2V-W21" autocomplete="off">
              <svg width="14" height="14" viewbox="0 0 28 28" xmlns="http://www.w3.org/2000/svg" class="search-icon"><title>Search</title>
<g fill-rule="nonzero"><path d="M17.332 20.735c-5.537 0-10-4.6-10-10.247 0-5.646 4.463-10.247 10-10.247 5.536 0 10 4.601 10 10.247s-4.464 10.247-10 10.247zm0-4c3.3 0 6-2.783 6-6.247 0-3.463-2.7-6.247-6-6.247s-6 2.784-6 6.247c0 3.464 2.7 6.247 6 6.247z"></path><path d="M11.672 13.791L.192 25.271 3.02 28.1 14.5 16.62z"></path></g></svg>
            </div>
            <div class="js-search-results search-results-wrap"></div>
          </div>
          
          
        </div>
        <div class="page">
          
            
              <nav class="breadcrumb-nav">
                <ol class="breadcrumb-nav-list">
                  
                    <li class="breadcrumb-nav-list-item"><a href="http://localhost:4000/VNA2V-W21-handouts/lab9/">Lab 9</a></li>
                  
                  <li class="breadcrumb-nav-list-item"><span>Exercises</span></li>
                </ol>
              </nav>
            
          
          <div id="main-content" class="page-content" role="main">
            
              <h1 class="no_toc text-delta fs-9" id="exercises">
        
        
          <a href="#exercises" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> Exercises
        
        
      </h1>

<ol id="markdown-toc">
  <li>
<a href="#submission" id="markdown-toc-submission">Submission</a>    <ol>
      <li><a href="#individual" id="markdown-toc-individual">Individual</a></li>
      <li><a href="#team" id="markdown-toc-team">Team</a></li>
      <li><a href="#deadline" id="markdown-toc-deadline">Deadline</a></li>
    </ol>
  </li>
  <li>
<a href="#-individual" id="markdown-toc--individual">üë§ Individual</a>    <ol>
      <li><a href="#-deliverable-1---spy-game-20-pts" id="markdown-toc--deliverable-1---spy-game-20-pts">üì® Deliverable 1 - Spy Game [20 pts]</a></li>
      <li><a href="#-deliverable-2---well-begun-is-half-done-10-pts" id="markdown-toc--deliverable-2---well-begun-is-half-done-10-pts">üì® Deliverable 2 - Well-begun is Half Done [10 pts]</a></li>
      <li><a href="#-deliverable-3---feature-based-methods-for-slam-10-pts" id="markdown-toc--deliverable-3---feature-based-methods-for-slam-10-pts">üì® Deliverable 3 - Feature-based methods for SLAM [10 pts]</a></li>
      <li><a href="#-deliverable-4-optional---direct-methods-for-slam-5-pts" id="markdown-toc--deliverable-4-optional---direct-methods-for-slam-5-pts">üì® Deliverable 4 [Optional] - Direct methods for SLAM [+5 pts]</a></li>
      <li><a href="#-deliverable-5-optional---from-landmark-based-slam-to-rotation-estimation-15-pts" id="markdown-toc--deliverable-5-optional---from-landmark-based-slam-to-rotation-estimation-15-pts">üì® Deliverable 5 [Optional] - From landmark-based SLAM to rotation estimation [+15 pts]</a></li>
    </ol>
  </li>
  <li>
<a href="#-team" id="markdown-toc--team">üë• Team</a>    <ol>
      <li><a href="#orb-slam" id="markdown-toc-orb-slam">ORB-SLAM</a></li>
      <li><a href="#kimera" id="markdown-toc-kimera">Kimera</a></li>
      <li><a href="#-deliverable-6---performance-comparison-60-pts" id="markdown-toc--deliverable-6---performance-comparison-60-pts">üì® Deliverable 6 - Performance Comparison [60 pts]</a></li>
      <li><a href="#-optional-deliverable-7---ldso-10-pts" id="markdown-toc--optional-deliverable-7---ldso-10-pts">üì® [Optional] Deliverable 7 - LDSO [+10 pts]</a></li>
      <li><a href="#summary-of-team-deliverables" id="markdown-toc-summary-of-team-deliverables">Summary of Team Deliverables</a></li>
    </ol>
  </li>
</ol>
    
      <h1 id="submission">
        
        
          <a href="#submission" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> Submission
        
        
      </h1>

<p>In this lab, there are 6 deliverables. Deliverables 1 to 5 will require pen and
paper and are considered individual tasks, while Deliverable 6 is a team task
which requires programming.</p>

<p>To submit your solutions create a folder called <code class="highlighter-rouge">lab9</code> and push to your
repository with your answers (it can be plain text, markdown, pdf or whatever
other format is reasonably easy to read).</p>
    
      <h3 id="individual">
        
        
          <a href="#individual" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> Individual
        
        
      </h3>

<p>Please push the deliverables into your personal repository, for math-related
questions LaTeX is preferred but handwritten is accepted too.</p>
    
      <h3 id="team">
        
        
          <a href="#team" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> Team
        
        
      </h3>

<p>Please push the source code for the entire package to the folder <code class="highlighter-rouge">lab9</code> of the
team repository. For the plots and discussion questions, please push a PDF to
the <code class="highlighter-rouge">lab9</code> folder of your team repository.</p>
    
      <h3 id="deadline">
        
        
          <a href="#deadline" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> Deadline
        
        
      </h3>

<p><strong>Deadline:</strong> the VNA2V staff will clone your repository on <strong>November 10th</strong> at midnight.</p>
    
      <h1 id="-individual">
        
        
          <a href="#-individual" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> üë§ Individual
        
        
      </h1>
    
      <h2 id="-deliverable-1---spy-game-20-pts">
        
        
          <a href="#-deliverable-1---spy-game-20-pts" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> üì® Deliverable 1 - Spy Game [20 pts]
        
        
      </h2>

<p>Consider the following
<a href="https://www.mathworks.com/help/matlab/ref/spy.html" target="_blank" rel="noopener noreferrer">spy</a>-style plot of an
information matrix (i.e., coefficient matrix in Gauss-Newton‚Äôs normal equations) 
for a landmark-based SLAM problem where dark cells correspond to non-zero blocks:</p>

<p><img src="http://localhost:4000/VNA2V-W21-handouts/assets/images/lab9/spy_game2.png" alt="Spy Plot" class="mx-auto d-block"></p>

<p>Assuming robot poses are stored
sequentially, answer the following questions:</p>

<ol>
  <li>How many robot poses exist in this problem?</li>
  <li>How many landmarks exist in the map?</li>
  <li>How many landmark have been observed by the current (last) pose?</li>
  <li>Which pose has observed the most number of landmark?</li>
  <li>What poses have observed the 2nd landmark?</li>
  <li>Predict the sparsity pattern of the information matrix after marginalizing
out the 2nd feature.</li>
  <li>Predict the sparsity pattern of the information matrix after marginalizing
out past poses (i.e., only retaining the last pose).</li>
  <li>Marginalizing out which variable (chosen among both poses or landmarks) would
preserve the sparsity pattern of the information matrix?</li>
  <li>The following figures illustrate the robot (poses-poses) block of the
 information matrix obtained after marginalizing out (eliminating) all
 landmarks in bundle adjustment in two different datasets. What can you say
 about these datasets (e.g., was robot exploring a large building? Or perhaps
 it was surveying a small room? etc) given the spy images below?</li>
</ol>

<p><img src="http://localhost:4000/VNA2V-W21-handouts/assets/images/lab9/spy_game1.png" alt="Spy Comparison" class="mx-auto d-block"></p>
    
      <h2 id="-deliverable-2---well-begun-is-half-done-10-pts">
        
        
          <a href="#-deliverable-2---well-begun-is-half-done-10-pts" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> üì® Deliverable 2 - Well-begun is Half Done [10 pts]
        
        
      </h2>

<p>Pose graph optimization is a non-convex problem. Therefore, iterative solvers
require a (good) initial guess to converge to the right solution. Typically, one
initializes nonlinear solvers (e.g., Gauss-Newton) from the odometric estimate
obtained by setting the first pose to the identity and chaining the odometric
measurements in the pose graph.</p>

<p>Considering that chaining more relative pose measurements (either odometry or
loop closures) accumulates more noise (and provides worse initialization),
propose a more accurate initialization method that also sets the first pose to
the identity but chains measurements in the pose graph in a more effective way.
A 1-sentence description and rationale for the proposed approach suffices.</p>

<blockquote>
  <p>Hint: consider a graph with an arbitrary topology. Also, think about the
problem as a graph, where you fix the ‚Äúroot‚Äù node and initialize each node by
chaining the edges from the root to that node..</p>
</blockquote>
    
      <h2 id="-deliverable-3---feature-based-methods-for-slam-10-pts">
        
        
          <a href="#-deliverable-3---feature-based-methods-for-slam-10-pts" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> üì® Deliverable 3 - Feature-based methods for SLAM [10 pts]
        
        
      </h2>

<p>Read the ORB-SLAM paper (available
<a href="https://drive.google.com/open?id=10y8nTvUVB9C-hSaZmtX6772qTZt0LnX-" target="_blank" rel="noopener noreferrer">here</a>) and
answer the following questions:</p>

<ol>
  <li>Provide a 1 sentence description of each module used by ORB-SLAM (Fig. 1 in
the paper can be a good starting point).</li>
  <li>Consider the case in which the place recognition module provides an incorrect
loop closure. How does ORB-SLAM check that each loop closure is correct? What
happens if an incorrect loop closure is included in the pose-graph
optimization module?</li>
</ol>
    
      <h2 id="-deliverable-4-optional---direct-methods-for-slam-5-pts">
        
        
          <a href="#-deliverable-4-optional---direct-methods-for-slam-5-pts" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> üì® Deliverable 4 [Optional] - Direct methods for SLAM [+5 pts]
        
        
      </h2>

<p>Read the LSD-SLAM paper (available
<a href="https://drive.google.com/open?id=1Wjm0sp0U0SQ9gwZ6FTyVxOiruMGR2qwp" target="_blank" rel="noopener noreferrer">here</a>, see
also the introduction below before reading the paper) and answer the following
questions:</p>

<ol>
  <li>Provide a 1 sentence description of each module used by LSD-SLAM and outline similarities and differences with respect to ORB-SLAM.</li>
  <li>Which approach (between feature-based or direct) is expected to be more
robust to changes in illumination or occlusions? Motivate your answer.</li>
</ol>

<p>LSD-SLAM is a <em>direct method</em> for SLAM, as opposed to <em>feature-based</em> methods we
have worked with previously. As you know, feature-based methods detect and match
features (keypoints) in each image and then use 2-view geometry (and possibly
bundle adjustment) to estimate the motion of the robot. Direct methods are
different in the fact that they do not extract features, but can be easily
understood using the material presented in class.</p>

<p>In particular, the main difference is the way the 2-view geometry is solved. In
feature-based approaches one uses RANSAC and a minimal solver (e.g., the 5-point
method) to infer the motion from feature correspondences. In direct methods,
instead, one tries to estimate the relative pose between consecutive frames by
minimizing directly the mismatch of the pixel intensities between two images:</p>

<p>\[ E(\xi) = \sum_i (I_{ref}(p_i) - I(\omega (p_i, D_{ref}(p_i), \xi)))^2
= r_i(\xi)^2
\]</p>

<p>Where the objective looks for a pose $\xi$ (between the last frame $I_{ref}$
 and the current frame $I$ that minimizes the mismatch between the intensity
 $I_{ref}(p_i)$ at pixel $p_i$ for each pixel $p_i$ in the image, and intensity
 of the corresponding pixel in the current image $I$. How do we retrieve the
 pixel corresponding to $p_i$ in the current image $I$? In other words, what is
 this term?:</p>

<p>\[ I(\omega (p_i, D_{ref}(p_i), \xi))\]</p>

<p>It seems mysterious, but it‚Äôs nothing new: this simply represents a perspective
projection. More specifically, given a pixel $p_i$, if we know the corresponding
depth $D_{ref}(p_i)$ we can get a 3D point that we can then project to the
current camera as a function of the relative pose. The $\omega$ is typically
called a <strong>warp function</strong> since it ‚Äúwarps‚Äù a pixel in the previous frame
$I_{ref}$ into a pixel at the current frame $I$. What is the catch? Well.. the
depth $D_{ref}$ is unknown in practice, so you can only optimize $E(\xi)$ if at
a previous step you have some triangulation of the points in the image. In
direct methods, therefore these is typically an ‚Äúinitialization step‚Äù: you use
feature-based methods to estimate the poses of the first few frames and
triangulate the corresponding points, and then you can use the optimization of
$E(\xi)$ to estimate later poses. The objective function $E(\xi)$ is called the
<em>photometric error</em>, which quantifies the difference in the pixel appearance
in consecutive frames.</p>

<p><strong><span style="text-decoration:underline;">NOTE</span></strong>:
<a href="https://drive.google.com/open?id=1YuVjojj3CsVFcOBo-IqUVELluz1vw9-t" target="_blank" rel="noopener noreferrer">LDSO</a>
(Direct Sparse Odometry with Loop Closures) which you are going to use in Part 2
of this handout is simply an evolution of LSD-SLAM (by the same authors). We are
suggesting you to read the LSD-SLAM paper since it provides a simpler
introduction to direct methods, while we decided to use LDSO for the
experimental part since it comes with a more recent implementation.</p>
    
      <h2 id="-deliverable-5-optional---from-landmark-based-slam-to-rotation-estimation-15-pts">
        
        
          <a href="#-deliverable-5-optional---from-landmark-based-slam-to-rotation-estimation-15-pts" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> üì® Deliverable 5 [Optional] - From landmark-based SLAM to rotation estimation [+15 pts]
        
        
      </h2>

<p>Consider the following landmark-based SLAM problem:</p>

<p>\[ \min_{t_i \in \mathbb{R}^3,\ R_i \in \SO{3},\ p_i \in \mathbb{R}^3} \sum_{(i,k) \in \mathcal{E}_l} || R_i^T(p_k - t_i) - \bar{p}_{ik} ||_2^2 + \sum_{(i,j) \in \mathcal{E}_o} ||R_i^T(t_j - t_i) - \bar{t}_{ij}||_2^2 + ||R_j - R_i\bar{R}_{ij}||_F^2 \]</p>

<p>Where the goal is to compute the poses of the robot $(t_i, R_i),\ i=1,\ldots,N$
and the positions of point-landmarks $p_k, k= 1, \ldots, M$ given odometric
measurements $(\bar{t}_{ij}, \bar{R}_{ij})$ for each odometric edge $(i,j) \in
\mathcal{E}_o$ (here $\mathcal{E}_o$ denotes the set of odometric edges), and
landmark observations $\bar{p}_{ik}$ of landmark $k$ from pose $i$ for each
observation edge $(i,k) \in \mathcal{E}_l$ (here $\mathcal{E}_l$ denotes the set of
pose-landmark edges).</p>

<ul>
  <li>Prove the following claim: ‚ÄúThe optimization problem (1) can be rewritten as
 a nonlinear optimization over the rotations $R_i,\ i=1,\ldots,N$ only.‚Äù Provide an expression of the resulting rotation-only problem to support the proof.</li>
</ul>

<blockquote>
  <p>Hint: (i) Euclidean norm is invariant under rotations, and (ii) translations/positions variables appear ‚Ä¶!
Consider also using a compact matrix notation to rewrite the sums in the cost
function otherwise it will be tough to get an expression of the rotation-only
problem</p>
</blockquote>

<ul>
  <li>The elimination of variables discussed at the previous point largely reduces
 the size of the optimization problem (from 6N+3L variables to 3N variables).
 However, the rotation problem is not necessarily faster to solve. Discuss
 what can make the rotation-only problem more computationally-demanding to
 solve.</li>
</ul>

<blockquote>
  <p>Hint: What property makes optimization-based SLAM algorithms fast to solve?</p>
</blockquote>
    
      <h1 id="-team">
        
        
          <a href="#-team" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> üë• Team
        
        
      </h1>
    
      <h2 id="orb-slam">
        
        
          <a href="#orb-slam" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> ORB-SLAM
        
        
      </h2>

<p>By now you should have ORB-SLAM installed, and you should be able to make it run
on the EuRoC dataset that we downloaded previously. You might be already
familiar with the visualization window of ORB-SLAM:</p>

<p><img src="http://localhost:4000/VNA2V-W21-handouts/assets/images/lab9/orbslam.png" alt="ORBSLAM Viz" class="mx-auto d-block"></p>

<p>Let us now use ORB-SLAM to estimate the trajectory of the camera. For that, run
ORB-SLAM for the whole <code class="highlighter-rouge">MH_01_easy</code> EuRoC sequence (<a href="http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/machine_hall/MH_01_easy/MH_01_easy.bag" target="_blank" rel="noopener noreferrer">available
here</a>)
and wait for the <code class="highlighter-rouge">KeyFrameTrajectory_TUM_Format.txt</code> file, which contains the
estimated trajectory, to be generated (it will be generated when you <code class="highlighter-rouge">ctrl+c</code>
the process, which you should only do at the end of the rosbag).</p>
    
      <h2 id="kimera">
        
        
          <a href="#kimera" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> Kimera
        
        
      </h2>

<p><a href="http://web.mit.edu/sparklab/2019/10/13/Kimera__an_Open-Source_Library_for_Real-Time_Metric-Semantic_Localization_and_Mapping.html" target="_blank" rel="noopener noreferrer">Kimera</a>
is an open-source library for real-time SLAM (<a href="https://arxiv.org/pdf/1910.02490.pdf" target="_blank" rel="noopener noreferrer">paper
here</a>). Let‚Äôs install Kimera and try out
its visual odometry system on the EuRoC dataset. Clone the <a href="https://github.com/MIT-SPARK/Kimera-VIO-ROS" target="_blank" rel="noopener noreferrer">Github
repository</a> into a catkin
workspace, following the instructions in the README for installation. Note: this
will build GTSAM, OpenCV, and OpenGV, so if you haven‚Äôt already built these,
this might take a while (around 15 minutes), so this is a good time to grab a
coffee.</p>

<p>In general, you can build Kimera as follows:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> ~/vna2v_ws/src/
git clone git@github.com:MIT-SPARK/Kimera-VIO-ROS.git
<span class="nb">sudo </span>apt-get update
<span class="nb">sudo </span>apt-get <span class="nb">install</span> <span class="nt">-y</span> <span class="nt">--no-install-recommends</span> apt-utils
<span class="nb">sudo </span>apt-get <span class="nb">install</span> <span class="nt">-y</span> <span class="se">\</span>
      cmake build-essential unzip pkg-config autoconf <span class="se">\</span>
      libboost-all-dev <span class="se">\</span>
      libjpeg-dev libpng-dev libtiff-dev <span class="se">\</span>
      libvtk6-dev libgtk-3-dev <span class="se">\</span>
      libatlas-base-dev gfortran <span class="se">\</span>
      libparmetis-dev <span class="se">\</span>
      python-wstool python-catkin-tools
<span class="nb">sudo </span>apt-get <span class="nb">install </span>libtbb-dev
wstool merge Kimera-VIO-ROS/install/kimera_vio_ros_ssh.rosinstall
wstool update
<span class="c"># Here you might get some messages from wstool </span>
<span class="c"># Just make sure you don't duplicate the packages </span>
catkin build kimera_vio_ros
<span class="nb">source</span> ~/vna2v_ws/devel/setup.bash
</code></pre></div></div>

<p>You can run ‚Äúonline‚Äù or ‚Äúoffline‚Äù on the EuRoC dataset bag file (see
instructions in the Kimera README). In ‚Äúonline‚Äù mode, Kimera will subscribe to
incoming images (expecting topic names to be in the correct EuRoC bag format),
which will be provided when you separately <code class="highlighter-rouge">rosbag play</code> the appropriate bag
file, and output trajectory estimates as it runs. In ‚Äúoffline‚Äù mode, Kimera will
read directly from the bag file, processing messages one at a time.</p>

<p>To run online, simply run:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>roslaunch kimera_vio_ros kimera_vio_ros_euroc.launch
</code></pre></div></div>
<p>Then, in another terminal run RViZ:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rviz <span class="nt">-d</span> <span class="si">$(</span>rospack find kimera_vio_ros<span class="si">)</span>/rviz/kimera_vio_euroc.rviz
</code></pre></div></div>
<p>Lastly, play the EuRoC <code class="highlighter-rouge">MH_01_easy</code> bag file:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rosbag play <span class="nt">--clock</span> /PATH/TO/EUROC_ROSBAG
</code></pre></div></div>

<p>Your RViZ window should now look something like this one:</p>

<p><img src="http://localhost:4000/VNA2V-W21-handouts/assets/images/lab9/kimera_vio.png" alt="Kimera RViz" class="mx-auto d-block"></p>

<p>Check out an example of Kimera running on one of the EuRoC datasets (V1_01)
<a href="https://www.dropbox.com/s/ynwilnrufjlt9dx/kimera_output.mp4?dl=0" target="_blank" rel="noopener noreferrer">here</a>.</p>

<p>By default, Kimera will place the output data in the <code class="highlighter-rouge">Kimera-VIO-ROS/output_logs/Euroc</code>
directory. Look in this directory for the file <code class="highlighter-rouge">traj_vio.csv</code>. These are
the state estimates provided by the VIO system in Euroc format.</p>
    
      <h2 id="-deliverable-6---performance-comparison-60-pts">
        
        
          <a href="#-deliverable-6---performance-comparison-60-pts" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> üì® Deliverable 6 - Performance Comparison [60 pts]
        
        
      </h2>

<p>Download and install <a href="https://github.com/MichaelGrupp/evo" target="_blank" rel="noopener noreferrer">the EVO repo</a>, which
allows us to evaluate the quality of trajectory estimates. Feel free to read
the useful Wiki page, Readme, and even Jupyter notebooks! that the repo
provides.</p>

<p>For Kimera, you will want to delete the first row of the <code class="highlighter-rouge">traj_vio.csv</code>
file, as <code class="highlighter-rouge">evo</code> doesn‚Äôt like the headers in this row. Note also that the Kimera poses are provided in EuRoC format, while the ORB-SLAM poses are provided in TUM format. To convert the Kimera poses to TUM format, simply run:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>evo_traj euroc traj_vio.csv <span class="nt">--save_as_tum</span>
</code></pre></div></div>

<p>Now you can use the <code class="highlighter-rouge">evo_traj</code> tool to plot both trajectories for <code class="highlighter-rouge">MH_01_easy</code>.
Report this plot and comment on the differences that you see between
trajectories:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>evo_traj tum KeyFrameTrajectory_TUM_Format.txt traj_vio.tum <span class="nt">--plot</span>
</code></pre></div></div>

<p>(Specify the correct path for each of your trajectory files)</p>

<p>In general, the trajectories output from different SLAM systems may use
different conventions for the world frame, so we have to align the trajectories
in order to compare them. This amounts to solving for the optimal (in the sense
of having the <em>best fit</em> to the ground truth) $\SE{3}$ transformation of the
estimated trajectory. We‚Äôve discussed these types of problems at length in
class, and we don‚Äôt ask you to implement the alignment process in this lab.
Fortunately, tools like <code class="highlighter-rouge">evo</code> have already provided efficient implementations
that do this for us. Let‚Äôs compare to the ground-truth pose data from the EuRoC
dataset next, taking into consideration this pose alignment. Grab the
ground-truth pose data <code class="highlighter-rouge">data.csv</code> from the <code class="highlighter-rouge">MH_01_easy.zip</code> file on the EuRoC
website
(<a href="http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/machine_hall/MH_01_easy/MH_01_easy.zip" target="_blank" rel="noopener noreferrer">here</a>).
The <code class="highlighter-rouge">data.csv</code> file is in the EuRoC format, so like with the Kimera data, first
use <code class="highlighter-rouge">evo_traj</code> to convert to TUM format. Next, run:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>evo_traj tum KeyFrameTrajectory_TUM_Format.txt traj_vio.tum <span class="nt">--ref</span> data.tum <span class="nt">--plot</span> <span class="nt">--align</span>
</code></pre></div></div>
<p>Report this plot and comment on the differences that you see between
trajectories.</p>
    
      <h2 id="-optional-deliverable-7---ldso-10-pts">
        
        
          <a href="#-optional-deliverable-7---ldso-10-pts" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> üì® [Optional] Deliverable 7 - LDSO [+10 pts]
        
        
      </h2>

<p>Based on LSD-SLAM, LDSO is a direct, yet sparse, formulation for SLAM. It is
also capable of handling loop closures.</p>

<p>Let us install the code for LDSO, to do that follow the <code class="highlighter-rouge">Readme.md</code> in the
<a href="https://github.com/tum-vision/LDSO" target="_blank" rel="noopener noreferrer">GitHub repository</a>.</p>

<p>In the same <code class="highlighter-rouge">Readme.md</code> there is an example on how to run this pipeline for the
EuRoC dataset. Detailed below for convenience:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> ~/code/LDSO
./bin/run_dso_euroc <span class="nv">preset</span><span class="o">=</span>0 <span class="nv">files</span><span class="o">=</span>/my_path_to_data/EuRoC/MH_01_easy/mav0/cam0
</code></pre></div></div>

<p>(Replace <code class="highlighter-rouge">my_path_to_data</code> with your path to the data).</p>

<p>At this point, you should be able to see the following window:</p>

<p><img src="http://localhost:4000/VNA2V-W21-handouts/assets/images/lab9/ldso.png" alt="LDSO Viz" class="mx-auto d-block"></p>

<p>LDSO will output by default the results in ‚Äú<code class="highlighter-rouge">./results.txt</code>‚Äù file in the same
folder where you ran the command. Alternatively, you can specify the output
folder by setting the flag <code class="highlighter-rouge">output</code> in the command line.</p>

<p>The results file will only be generated once the sequence has all been
processed. Let now the pipeline run over the <code class="highlighter-rouge">MH_01_easy </code>sequence and ensure
that the results file is present.</p>

<p>First of all, the output from LDSO seems to be wrongly formatted, as it has
spurious backspaces that should not be there. To remove those, you can do it as
you prefer but if you use vim you could do the following:
    1. Install vim: sudo apt-get install vim
    2. Open the file in vim: vim results.txt
    3. Type the following keys:
        1. :
        2. <code class="highlighter-rouge">%s/\ *\ /\ /g</code>
        3. :wq</p>

<p>It might look like a weird command, but it is simply saying, for all the lines
in the file (%), substitute (s/) all sequences of backspaces (\ *\ ) for (/)
only one backspace (\ ) for any repetition in the line (/g).</p>

<p>Plot the trajectory produced by LDSO with those of Kimera and ORB-SLAM and
comment on any differences in the trajectories</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>evo_traj tum KeyFrameTrajectory_TUM_Format.txt traj_vio.tum results.txt <span class="nt">--ref</span> data.tum <span class="nt">--plot</span> <span class="nt">--align</span> <span class="nt">--correct_scale</span>
</code></pre></div></div>

<p>Note, here since <code class="highlighter-rouge">LDSO</code> is a monocular method there is no way of recovering the
true scale of the scene, so we use the <code class="highlighter-rouge">--correct_scale</code> flag for <code class="highlighter-rouge">evo</code> to
estimate not just the $\SE{3}$ alignment of the predicted trajectory to the
ground-truth trajectory, but also the global scale factor.</p>

<p>Here‚Äôs an example of the aligned trajectories plotted in the x-y plane (you can
add the flag <code class="highlighter-rouge">--plot_mode xy</code> to compare with this plot):</p>

<p><img src="http://localhost:4000/VNA2V-W21-handouts/assets/images/lab9/example_kimera.png" alt="Kimera Output" class="mx-auto d-block"></p>
    
      <h2 id="summary-of-team-deliverables">
        
        
          <a href="#summary-of-team-deliverables" class="anchor-heading"><svg class="d-inline-block v-align-middle" viewbox="0 0 16 16" version="1.1" width="18" height="18" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a> Summary of Team Deliverables
        
        
      </h2>
<ol>
  <li>Comparison plots of ORB-SLAM and Kimera on <code class="highlighter-rouge">MH_01_easy</code> with comments on differences in trajectories</li>
  <li>Plot of ORB-SLAM and Kimera trajectories on <code class="highlighter-rouge">MH_01_easy</code> aligned with
ground-truth with comments on any differences in the trajectories</li>
  <li>[Optional] Plot of ORB-SLAM + Kimera + LDSO (aligned and scale corrected with
ground-truth) with comments on any differences in trajectories</li>
</ol>
            

          

          
            <hr>
            <footer role="contentinfo">
              <p class="text-small text-grey-dk-000 mb-0">Enabled with the generous support of the MIT SPARK Lab. Copyright ¬© MIT SPARK Lab and Vasileios Tzoumas of the University of Michigan. Using <a href="https://github.com/pmarsceill/just-the-docs/tree/master" target="_blank" rel="noopener noreferrer">Just the docs</a> theme.</p>
            </footer>
          

        </div>
      </div>
    </div>
  </div>

</div></body>
</html>
